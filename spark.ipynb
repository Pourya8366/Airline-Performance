{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Airline Performance\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 15:50:16,191 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "#read records and make sql dataframe\n",
    "flights = spark.read.option(\"header\",True).csv('hdfs:///airline_performance/airline_2m.csv')\n",
    "flights.createOrReplaceTempView(\"flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find airports as vertices in graph\n",
    "airports = spark.sql(\"select Origin as id from flights union select Dest from flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(origin airport, dest airport, airline) key stats as edges\n",
    "stats = spark.sql(\"select Origin as src, Dest as dst, Reporting_Airline as airline, Year, Quarter, Month, DayofMonth, DayOfWeek, Origin, OriginStateName, OriginCityName, Dest, DestStateName, DestCityName, cast(DepDelay as int) as Departure_delay, cast(ArrDelay as int) as Arrival_Delay, DepTimeBlk, cast(Cancelled as int) as Cancelled, cast(CRSElapsedTime as int) as CRSElapsed_Time, cast(ActualElapsedTime as int) as Actual_Elapsed_Time, Distance from flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "\n",
    "graph = GraphFrame(airports, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. unique airports\n",
    "# 1. \n",
    "graph.vertices.count\n",
    "# 2.\n",
    "airports.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trivial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which Year?\n",
    "yearly_delay = spark.sql(\"select Year, avg(ArrDelay) as Arrival_Delay, sum(Cancelled) as Total_Cancelled from stats group by Year order by Arrival_Delay desc, Total_Cancelled desc\")\n",
    "\n",
    "# which month of each year?\n",
    "monthly_delay = spark.sql(\"select Year, Month, avg(ArrDelay) as Arrival_Delay, sum(Cancelled) as Total_Cancelled from stats group by Year, Month order by Arrival_Delay desc, Total_Cancelled desc\")\n",
    "\n",
    "\n",
    "# which day of month of each year?\n",
    "daily_delay = spark.sql(\"select Year, Month, DayofMonth as Day, avg(ArrDelay) as Arrival_Delay, sum(Cancelled) as Total_Cancelled from stats group by Year, Month, Day order by Arrival_Delay desc, Total_Cancelled desc\")\n",
    "\n",
    "# which day of week of each month of each year?\n",
    "daily_delay = spark.sql(\"select Year, Month, DayOfWeek, avg(ArrDelay) as Arrival_Delay, sum(Cancelled) as Total_Cancelled from stats group by Year, Month, DayOfWeek order by Arrival_Delay desc, Total_Cancelled desc\")\n",
    "\n",
    "hourly_delay = spark.sql(\"select DepTimeBlk, avg(ArrDelay) as Arrival_Delay from stats group by DepTimeBlk order by Arrival_Delay desc\")\n",
    "\n",
    "# airline ranking, worst to best?\n",
    "airline_delay = spark.sql(\"select airline, avg(ArrDelay) as Arrival_Delay, sum(Cancelled) as Total_Cancelled from stats group by airline order by Arrival_Delay desc, Total_Cancelled desc\")\n",
    "\n",
    "airline_yearly = spark.sql(\"select Year, airline, avg(ArrDelay) as Arrival_Delay, sum(Cancelled) as Total_Cancelled from stats group by Year, airline \")\n",
    "\n",
    "# worst airline of the year (delay)?\n",
    "worst_airline_delay = spark.sql(\"select Year, airline, max(Arrival_Delay) as Arrival_Delay from airline_yearly group by Year\")\n",
    "\n",
    "# best airline of the year (delay)?\n",
    "best_airline_delay = spark.sql(\"select Year, airline, min(Arrival_Delay) as Arrival_Delay from airline_yearly group by Year\")\n",
    "\n",
    "# worst airline of the year (cancelled)?\n",
    "worst_airline_cancelled = spark.sql(\"select Year, airline, max(Total_Cancelled) as Total_Cancelled from airline_yearly group by Year\")\n",
    "\n",
    "# best airline of the year (cancelled)?\n",
    "best_airline_cancelled = spark.sql(\"select Year, airline, min(Total_Cancelled) as Total_Cancelled from airline_yearly group by Year\")\n",
    "\n",
    "# airline performance year, month (delay, cancelled)\n",
    "airline_performance = spark.sql(\"select Year, Month, airline, avg(ArrDelay) as Arrival_Delay, sum(Cancelled) as Total_Cancelled from stats group by Year, Month order by Arrival_Delay desc, Total_Cancelled desc\")\n",
    "\n",
    "# longest distance flights\n",
    "distance_longest = spark.sql(\"select src, dst, max(Distance) as Distance from stats group by src, dst order by Distance\")\n",
    "\n",
    "# airports with longest delays\n",
    "airports_delay = spark.sql(\"select src, dst, avg(Arrival_Delay) as Arrival_Delay from stats group by src, dst order by Arrival_Delay desc\")\n",
    "\n",
    "# top 10 most flights\n",
    "most_flights = spark.sql(\"select src, dst, count(*) as NumberOfFlights from stats group by src, dst order by NumberOfFlights desc\").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 busiest airports\n",
    "graph.degrees.orderBy(desc(\"degree\")).take(10)\n",
    "\n",
    "# bottom 10 busiest airports\n",
    "graph.degrees.orderBy(asc(\"degree\")).take(10)\n",
    "\n",
    "# Page rank algorithm\n",
    "result = graph.pageRank(resetProbability=0.15, tol=0.01)\n",
    "result.vertices.orderBy(desc(\"pagerank\")).take(10)\n",
    "\n",
    "# find airports which don't have direct flight\n",
    "subGraph = GraphFrame(graph.vertices, most_flights)\n",
    "no_directs = subGraph.find(\"(src)-[]->(middle); (middle)-[]->(dst); !(src)-[]->(dst)\").filter(\"src.id !=dst.id\")\n",
    "\n",
    "# Strongly Connected Components \n",
    "# find airports that are in a Component of graph\n",
    "scc = graph.stronglyConnectedComponents(maxIter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corellation\n",
    "- Between Delay and Distance\n",
    "- Between Delay and categorical columns\n",
    "  - Month\n",
    "  - DayofMonth\n",
    "  - DayOfWeek\n",
    "  - DepTimeBlk (Computer Reservation System (scheduled) time block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for making dummy variables for a column\n",
    "def corr(source_column, target_column):\n",
    "    categories = stats.select(target_column).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    exprs = [F.when(F.col(target_column) == category, 1).otherwise(0).alias(category)\n",
    "         for category in categories]\n",
    "\n",
    "    corr_DF = stats.select(source_column, *exprs)\n",
    "\n",
    "    corrs = []\n",
    "\n",
    "    for category in categories:\n",
    "        corrs.append(corr_DF.stat.corr(source_column, category))\n",
    "    \n",
    "    return corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.stat.corr(\"Distance\", \"Departure_delay\")\n",
    "\n",
    "stats.stat.corr(\"Distance\", \"Arrival_Delay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA (Analysis of Covariance) \n",
    "corr_list_month = corr(\"Departure_delay\", \"Month\")\n",
    "corr_list_day_month = corr(\"Departure_delay\", \"DayofMonth\")\n",
    "corr_list_day_week = corr(\"Departure_delay\", \"DayOfWeek\")\n",
    "corr_list_hour = corr(\"Departure_delay\", \"DepTimeBlk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.coalesce(1).write.csv(\"hdfs:///airline_performance/dest\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
