{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Using in Spark RDD\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Using in Spark DataFrame\n",
    "spark = SparkSession.builder.appName(\"Airline Performance\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#day of week delay mapper\n",
    "def delayMap(s):\n",
    "    flight = s.split(\",\")\n",
    "    delay = 0\n",
    "    if flight[49] == \"0.00\" and flight[51] == \"0.00\" and flight[44] != \"\":\n",
    "        delay = float(flight[44])\n",
    "        \n",
    "    return (flight[0].strip('\"'), flight[2].strip('\"'), flight[4].strip('\"')), delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark RDD\n",
    "%%timeit  -n1 -r1\n",
    "flights = sc.textFile(\"hdfs:///airline/dataset\")\n",
    "delays = flights.map(delayMap)\n",
    "sumCount = (0,0)\n",
    "sumCountRDD = delays.aggregateByKey(sumCount, lambda a,b: (a[0] + b, a[1] + 1), lambda a,b: (a[0] + b[0], a[1] + b[1]))\n",
    "delayAvg = sumCountRDD.mapValues(lambda v: v[0]/v[1])\n",
    "delayAvg = delayAvg.sortBy(lambda x: x[1], ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark InferSchema\n",
    "%%timeit -n1 -r1\n",
    "\n",
    "flights = spark.read.option(\"header\",True).option(\"inferSchema\" , True).csv('hdfs:///airline_performance/fft00')\n",
    "flights.createOrReplaceTempView(\"flights\")\n",
    "\n",
    "result = spark.sql(\"select Year, Month, DayOfWeek , avg(ArrDelay) as Arrival_Delay from flights group by Year, Month, DayOfWeek order by Arrival_Delay desc\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark Schema Specified\n",
    "%%timeit -n1 -r1\n",
    "schema = StructType([\n",
    "    StructField(\"Year\", StringType()),\n",
    "    StructField(\"Month\", StringType()),\n",
    "    StructField(\"DayOfWeek\", StringType()),\n",
    "    StructField(\"Arrival_Delay\", IntegerType())\n",
    "])\n",
    "\n",
    "flights = spark.read.csv('hdfs:///airline/dataset', header=True, schema=schema)\n",
    "flights.createOrReplaceTempView(\"flights\")\n",
    "\n",
    "result = spark.sql(\"select Year, Month, DayOfWeek , avg(Arrival_Delay) as Arrival_Delay from flights group by Year, Month, DayOfWeek order by Arrival_Delay desc\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark SQL\n",
    "%%timeit -n1 -r1\n",
    "flights = spark.read.option(\"header\",True).csv('hdfs:///airline_performance/fft00')\n",
    "flights.createOrReplaceTempView(\"flights\")\n",
    "\n",
    "result = spark.sql(\"select Year, Month, DayOfWeek , avg(cast(ArrDelay as int)) as Arrival_Delay from flights group by Year, Month, DayOfWeek order by Arrival_Delay desc\").collect()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
